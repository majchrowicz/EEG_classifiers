{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG resting state ML classification project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set directories\n",
    "# %cd D:/Programy/Anaconda3/Projects/EEG ML project # working directory\n",
    "%cd D:\n",
    "pkls = './Pickles/' # objects & variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see all output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load already prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load time-series data, non-averaged\n",
    "with open(pkls +'data_srf.pkl', 'rb') as handle:\n",
    "    srf = pickle.load(handle)\n",
    "srf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load time-series data, non-averaged - open & closed eyes (oc) sessions\n",
    "with open(pkls +'data_srf_oc.pkl', 'rb') as handle:\n",
    "    srf_oc = pickle.load(handle)\n",
    "srf_oc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load time-series data, non-averaged - video watching (vw) sessions\n",
    "with open(pkls +'data_srf_vw.pkl', 'rb') as handle:\n",
    "    srf_vw = pickle.load(handle)\n",
    "srf_vw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load averaged data with eeg only\n",
    "with open(pkls +'data_ave.pkl', 'rb') as handle:\n",
    "    ave = pickle.load(handle)\n",
    "ave.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load averaged data with eeg and questionnaire\n",
    "with open(pkls +'data_aveq.pkl', 'rb') as handle:\n",
    "    aveq = pickle.load(handle)\n",
    "aveq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load split sets, eeg only\n",
    "with open(pkls +'xy_train.pkl', 'rb') as handle:\n",
    "    x_train = pickle.load(handle)\n",
    "    y_train = pickle.load(handle)\n",
    "y_train.shape\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load split sets, eeg and questionnaire\n",
    "with open(pkls +'xy_train_q.pkl', 'rb') as handle:\n",
    "    x_train = pickle.load(handle)\n",
    "    y_train = pickle.load(handle)\n",
    "y_train.shape\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load x train reduced (post-PCA), eeg only\n",
    "with open(pkls +'x_train_reduced.pkl', 'rb') as handle:\n",
    "    x_train_reduced = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local computer\n",
    "%cd D:\n",
    "path_oc = Path('D:/Programy/Anaconda3/Projects/EEG ML project/Epochs clean/)\n",
    "path_vw = Path('D:/Programy/Anaconda3/Projects/EEG ML project/Epochs clean/video')\n",
    "epo_list_oc = list(path_oc.glob('*.fif'))\n",
    "epo_list_vw = list(path_vw.glob('*.fif'))\n",
    "len(epo_list_oc)\n",
    "len(epo_list_vw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote computer\n",
    "%cd D:\n",
    "path_oc = Path('D:/Marcin/OneDrive - Uniwersytet Jagielloński/Nauka/Projekty/2020.02 - Bartek ML EEG/Epochs clean/')\n",
    "path_vw = Path('D:/Marcin/OneDrive - Uniwersytet Jagielloński/Nauka/Projekty/2020.02 - Bartek ML EEG/Epochs clean/video')\n",
    "epo_list_oc = list(path_oc.glob('*.fif'))\n",
    "epo_list_vw = list(path_vw.glob('*.fif'))\n",
    "len(epo_list_oc)\n",
    "len(epo_list_vw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and wrangle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load open and closed eyes sessions\n",
    "srf_oc = pd.DataFrame();\n",
    "for file in epo_list_oc:\n",
    "    rf = mne.read_epochs(fname = file).to_data_frame();\n",
    "    rf['id'] = file.stem[:5];\n",
    "    rf.drop('Status', axis = 1, inplace = True)\n",
    "    display(rf.shape);\n",
    "    srf_oc = srf_oc.append(rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load video watching sessions\n",
    "srf_vw = pd.DataFrame();\n",
    "for file in epo_list_vw:\n",
    "    rf = mne.read_epochs(fname = file).to_data_frame();\n",
    "    rf['id'] = file.stem[6:11];\n",
    "    rf.drop('Status', axis = 1, inplace = True)\n",
    "    display(rf.shape);\n",
    "    srf_vw = srf_vw.append(rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview\n",
    "display(srf_oc)\n",
    "srf_oc.shape\n",
    "\n",
    "display(srf_vw)\n",
    "srf_vw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump to pickle - oc\n",
    "with open(pkls + 'data_srf_oc.pkl', 'wb') as handle:\n",
    "    pickle.dump(srf_oc, handle, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump to pickle - vw\n",
    "with open(pkls + 'data_srf_vw.pkl', 'wb') as handle:\n",
    "    pickle.dump(srf_vw, handle, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge sessions\n",
    "srf = srf_oc.append(srf_vw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump to pickle - merged\n",
    "with open(pkls + 'data_srf.pkl', 'wb') as handle:\n",
    "    pickle.dump(srf, handle, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group and re-index\n",
    "avep = srf.groupby(['condition', 'epoch', 'id']).mean() # group means\n",
    "\n",
    "avep = avep.rename_axis(columns = 'indx') # reset index\n",
    "avep.reset_index(inplace=True)\n",
    "\n",
    "avep.sort_values(by = ['id', 'epoch'], inplace=True) # sort and reindex\n",
    "avep.reset_index(inplace=True, drop=True)\n",
    "\n",
    "avep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count epochs per condition and id\n",
    "avep.groupby(['id', 'condition']).agg('count')['epoch']\n",
    "\n",
    "# mutate values\n",
    "avepc = avep.replace({'condition': {'CLOSED/LONG': 0, 'CLOSED/SHORT': 0, # CE\n",
    "                                    'OPEN/LONG': 1, 'OPEN/SHORT': 1,     # OE\n",
    "                                    'VIDEO': 2 }})                       # VW\n",
    "avepc.groupby(['condition']).agg('count')['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avepc.isnull().values.any()\n",
    "avepc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add questionnaire data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "arsq = pd.read_csv('./ARSQ2/eyes_opened_closed_factors.tsv', sep = '\\t')\n",
    "arsq.columns.values[0] = 'id'\n",
    "\n",
    "# merge\n",
    "avepcq = pd.merge(avepc, arsq, how = 'left', on = 'id') # left join on id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ids with no ARSQ data\n",
    "g = avepcq.groupby('id')['Discontinuity of mind']\n",
    "g = g.count().rsub(g.size(), axis = 0)\n",
    "g[g != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer for ARSQ missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy = 'median') # apply only to numeric columns\n",
    "\n",
    "apq = avepcq.loc[:, 'Discontinuity of mind':'Verbal thought']\n",
    "imputer.fit(apq) # stored in imputer.statistics_ (same as ap_num.median().values)\n",
    "\n",
    "ap_imp = imputer.transform(apq)\n",
    "ap_imp = pd.DataFrame(ap_imp, columns = apq.columns, index = apq.index) # add transformed features to Pandas df\n",
    "ap_imp\n",
    "ap_imp.shape\n",
    "\n",
    "avepcq = pd.merge(avepc, ap_imp, how = 'left', left_index=True, right_index=True) # left join on id\n",
    "avepcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check numbers\n",
    "avepcq.drop(['epoch', 'condition'], axis = 1).groupby(['id']).first()\n",
    "avepcq.drop(['epoch', 'condition'], axis = 1).groupby(['id']).mean()\n",
    "\n",
    "# avepcq.iloc[:, np.r_[2, 77:87]].groupby('id').median() # get median by id\n",
    "# avepcq.iloc[:, np.r_[2, 77:87]].iloc[1:10].median() # check grand medians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add markers features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open(pkls +'features_r3.pkl', 'rb') as handle:\n",
    "    ftrs = pickle.load(handle)\n",
    "    \n",
    "# count epochs per participant in each df\n",
    "# avepc.groupby(['id']).agg('count')['epoch']\n",
    "# ftrs.groupby(['id']).agg('count')['epoch'] \n",
    "\n",
    "ftrs\n",
    "ftrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # no questionnaire \n",
    "ave = pd.merge(avepc, ftrs, how = 'left', on = ['id', 'epoch'])\n",
    "ave = ave.drop(ave.loc[:, 'EXG1': 'HEOG'].columns, axis = 1) # drop EXT electrodes\n",
    "ave.shape\n",
    "# list(ave.columns.values) # view all columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with questionnaire\n",
    "aveq = pd.merge(avepcq, ftrs, how = 'left', on = ['id', 'epoch'])\n",
    "aveq = aveq.drop(aveq.loc[:, 'EXG1': 'HEOG'].columns, axis = 1) # drop EXT electrodes\n",
    "aveq.shape\n",
    "# list(aveq.columns.values) # view all columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave.info()\n",
    "# ave.describe()\n",
    "avepc.groupby(['condition']).agg('count')['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full data, no questionnaire\n",
    "with open(pkls + 'data_ave.pkl', 'wb') as handle:\n",
    "    pickle.dump(ave, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full data, with questionnaire\n",
    "with open(pkls + 'data_aveq.pkl', 'wb') as handle:\n",
    "    pickle.dump(aveq, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "\n",
    "# ave = aveq\n",
    "ave.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified split\n",
    "split = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(ave, ave['condition']):\n",
    "    strat_train_set = ave.loc[train_index]\n",
    "    strat_test_set = ave.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ratios after splitting\n",
    "ave['condition'].value_counts() / len(ave) # all data\n",
    "strat_train_set['condition'].value_counts() / len(strat_train_set)\n",
    "strat_test_set['condition'].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy train set, set aside test set\n",
    "ap = strat_train_set.copy().sort_index()\n",
    "ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X (features) and y (target)\n",
    "x_train_pre = ap.drop(['condition', 'id', 'epoch'], axis = 1) \n",
    "y_train = ap[['condition']]\n",
    "\n",
    "# encode target variable\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# encoder = LabelEncoder()\n",
    "# y_train_cat = y_train.apply(encoder.fit_transform).rename(columns={'condition':'condcat'})\n",
    "# y_train_labs = pd.concat([y_train, y_train_cat], axis = 1)\n",
    "# y_train_labs\n",
    "# y_train = y_train_labs.drop(['condition'], axis = 1).values.ravel()\n",
    "# y_train\n",
    "\n",
    "x_train_pre = x_train_pre.sort_index()\n",
    "y_train = y_train.sort_index()\n",
    "# pd.merge(x_train, y_train, left_index=True, right_index=True) # to merge back\n",
    "\n",
    "x_train_pre\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # looking for correlations\n",
    "# from pandas.plotting import scatter_matrix\n",
    "\n",
    "# corr_matrix = ap.corr()\n",
    "# corr_matrix\n",
    "\n",
    "# attributes = ['Fz', 'Iz', 'Oz', 'POz', 'Pz', 'CPz', 'Fpz'] # failed: attributes = ap.iloc[:,0:10]\n",
    "# scatter_matrix(ap[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer # if joining numerical and categorical pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical pipeline\n",
    "\n",
    "# ap_num = ap.drop(['id', 'condition', 'epoch'], axis = 1)\n",
    "# num_attribs = list(ap_num)\n",
    "\n",
    "num_pipeline = Pipeline([ # construct pipeline\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('scaler', StandardScaler()), # consider RobustScaler if there are many outliers\n",
    "    ])\n",
    "\n",
    "x_train = num_pipeline.fit_transform(x_train_pre) # apply pipeline\n",
    "\n",
    "type(x_train) # see\n",
    "x_train_pre\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to pandas dataframe\n",
    "x_train = pd.DataFrame.from_records(x_train) # back to df\n",
    "x_train.columns = x_train_pre.columns # copy column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(pkls + 'xy_train.pkl', 'wb') as handle:\n",
    "    pickle.dump(x_train, handle)\n",
    "    pickle.dump(y_train, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save with quesionnaire\n",
    "with open(pkls + 'xy_train_q.pkl', 'wb') as handle:\n",
    "    pickle.dump(x_train, handle)\n",
    "    pickle.dump(y_train, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components =  0.98) # 2.5 SD = 0.9876, 3 SD = 0.9973\n",
    "x_train_reduced = pca.fit_transform(x_train)\n",
    "\n",
    "len(x_train.columns) # all components\n",
    "len(pca.explained_variance_ratio_) # preserved components/dimensions\n",
    "sum(pca.explained_variance_ratio_) # preserved variance\n",
    "len(pca.explained_variance_ratio_)/len(x_train.columns) # % of preserved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkls + 'x_train_reduced.pkl', 'wb') as handle:\n",
    "    pickle.dump(x_train_reduced, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.close()\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cumsum)\n",
    "# x_train.sort_index().cumsum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "x_reduced = rbf_pca.fit_transform(x_train)\n",
    "x_preimage = rbf_pca.inverse_transform(x_reduced)\n",
    "\n",
    "mean_squared_error(x_train, x_preimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(65,85,3, dtype = int)\n",
    "np.logspace(start = 1, stop = 100, num = 3)\n",
    "np.array([1,2])\n",
    "np.power(np.array([1,100]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'gamma': [1, 0.1, 0.01],\n",
    "#               'n_components': [65,75,85],\n",
    "              'kernel': ['rbf']}\n",
    "\n",
    "def my_scorer(estimator, x_train, y=None):\n",
    "    x_reduced = estimator.transform(x_train)\n",
    "    x_preimage = estimator.inverse_transform(x_reduced)\n",
    "    return mean_squared_error(x_train, x_preimage)\n",
    "\n",
    "grid_kpca = GridSearchCV(KernelPCA(n_components = 70, fit_inverse_transform=True), \n",
    "                         param_grid, scoring=my_scorer, cv = 2)\n",
    "\n",
    "grid_kpca.fit(x_train)\n",
    "grid_kpca.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "    ])\n",
    "\n",
    "param_grid = [{\n",
    "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "    }]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "grid_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
